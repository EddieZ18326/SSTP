{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "dataset = pd.read_csv('/Users/wzhang/Downloads/lake_erie_habs_W4_6_8_9_12_13_16_2013-2020.xlsx - HAB_data (1).csv')\n",
    "\n",
    "labels = dataset.iloc[:,16:].values\n",
    "\n",
    "vals = pd.DataFrame(dataset.iloc[:,4:15].values)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train,X_test,Y_train,Y_test = train_test_split(vals,labels,test_size=0.2,random_state=0)\n",
    "X_train = np.array(X_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-27 15:45:57.651737: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import LSTM, Dropout, Dense, BatchNormalization, GRU, LayerNormalization, MultiHeadAttention,TimeDistributed, Input, Flatten\n",
    "from sklearn.ensemble import RandomForestRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_GRU():\n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(GRU(units=100, return_sequences=True, input_shape=(X_train.shape[1], 1)))\n",
    "    model.add(Dropout(0.2))\n",
    "\n",
    "    model.add(GRU(units=100, return_sequences=True))\n",
    "    model.add(Dropout(0.2))\n",
    "\n",
    "    model.add(GRU(units=100))\n",
    "    model.add(Dropout(0.2))\n",
    "\n",
    "    model.add(Dense(units=1))\n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.0008), loss='mae')\n",
    "    model.fit(\n",
    "        X_train, Y_train, validation_split = 0.2, batch_size=24, epochs=2000\n",
    "    )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Lasso\n",
    "def create_lasso():\n",
    "    model = Lasso(alpha=100)\n",
    "    model.fit(X_train,Y_train)\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
    "def create_LSTM():\n",
    "\n",
    "    # Define the LSTM model\n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(LSTM(units=100, return_sequences=True, input_shape=(X_train.shape[1], 1)))\n",
    "    model.add(Dropout(0.2))\n",
    "\n",
    "    model.add(LSTM(units=100, return_sequences=True))\n",
    "    model.add(Dropout(0.2))\n",
    "\n",
    "    model.add(LSTM(units=100))\n",
    "    model.add(Dropout(0.2))\n",
    "\n",
    "    model.add(Dense(units=1))\n",
    "\n",
    "    model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "\n",
    "    # Compile the model\n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.00075), loss='mae')\n",
    "    # Fit the model to the training data\n",
    "    history = model.fit(X_train, Y_train, epochs=1000, batch_size=64, validation_data=(X_test, Y_test), shuffle=False)\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_mlp():\n",
    "    # Initialize MLP model\n",
    "    mlp_model = Sequential([\n",
    "        Dense(64, activation='relu', input_shape=(X_train.shape[1],)),\n",
    "        Dense(32, activation='relu'),\n",
    "        Dense(16, activation='relu'),\n",
    "        Dense(1)  # Output layer for regression task\n",
    "    ])\n",
    "\n",
    "    # Compile the model\n",
    "    mlp_model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate = 0.001), loss='mae')\n",
    "\n",
    "    # Print model summary\n",
    "    mlp_model.summary()\n",
    "\n",
    "    # Train the model\n",
    "    history = mlp_model.fit(X_train, Y_train, epochs=300, batch_size=32, validation_split=0.2)\n",
    "\n",
    "    # Evaluate model performance\n",
    "    return mlp_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_rf():\n",
    "    # Initialize Random Forest regressor\n",
    "    rf_model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "\n",
    "    # Train the model\n",
    "    rf_model.fit(X_train, Y_train)\n",
    "    return rf_model\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "def create_ridge():\n",
    "    model = Ridge(alpha=100)\n",
    "    model.fit(X_train,Y_train)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVR\n",
    "def create_SVM():\n",
    "    # Initialize SVM regressor\n",
    "    svm_model = SVR(kernel='rbf', C=10000, gamma='scale')\n",
    "\n",
    "    # Train the model\n",
    "    svm_model.fit(X_train, Y_train)\n",
    "    return svm_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │           <span style=\"color: #00af00; text-decoration-color: #00af00\">768</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">2,080</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)             │           <span style=\"color: #00af00; text-decoration-color: #00af00\">528</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │            <span style=\"color: #00af00; text-decoration-color: #00af00\">17</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │           \u001b[38;5;34m768\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)             │         \u001b[38;5;34m2,080\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_2 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m)             │           \u001b[38;5;34m528\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_3 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │            \u001b[38;5;34m17\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">3,393</span> (13.25 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m3,393\u001b[0m (13.25 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">3,393</span> (13.25 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m3,393\u001b[0m (13.25 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 23.3004 - val_loss: 23.6844\n",
      "Epoch 2/300\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 23.9553 - val_loss: 20.7076\n",
      "Epoch 3/300\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 21.5882 - val_loss: 18.7870\n",
      "Epoch 4/300\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 17.4006 - val_loss: 17.2153\n",
      "Epoch 5/300\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 17.5907 - val_loss: 16.5404\n",
      "Epoch 6/300\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 15.8747 - val_loss: 15.9586\n",
      "Epoch 7/300\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 16.0193 - val_loss: 15.0291\n",
      "Epoch 8/300\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 15.3345 - val_loss: 16.0471\n",
      "Epoch 9/300\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 15.4646 - val_loss: 14.5076\n",
      "Epoch 10/300\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 16.4450 - val_loss: 14.2712\n",
      "Epoch 11/300\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 15.0694 - val_loss: 13.8443\n",
      "Epoch 12/300\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 12.6752 - val_loss: 13.8478\n",
      "Epoch 13/300\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 15.7417 - val_loss: 14.3167\n",
      "Epoch 14/300\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 15.6927 - val_loss: 13.6369\n",
      "Epoch 15/300\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 12.5079 - val_loss: 13.5473\n",
      "Epoch 16/300\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 15.8263 - val_loss: 13.5438\n",
      "Epoch 17/300\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 13.8847 - val_loss: 13.6129\n",
      "Epoch 18/300\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 13.0443 - val_loss: 13.7424\n",
      "Epoch 19/300\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 12.7800 - val_loss: 13.4432\n",
      "Epoch 20/300\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 11.6697 - val_loss: 13.2086\n",
      "Epoch 21/300\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 14.7945 - val_loss: 14.0296\n",
      "Epoch 22/300\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 12.0548 - val_loss: 14.4579\n",
      "Epoch 23/300\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 13.3100 - val_loss: 13.4263\n",
      "Epoch 24/300\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 14.8964 - val_loss: 13.2278\n",
      "Epoch 25/300\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 13.7048 - val_loss: 13.3504\n",
      "Epoch 26/300\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 12.3846 - val_loss: 13.3120\n",
      "Epoch 27/300\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 13.6295 - val_loss: 13.3082\n",
      "Epoch 28/300\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 12.1924 - val_loss: 13.1557\n",
      "Epoch 29/300\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 12.3208 - val_loss: 13.9350\n",
      "Epoch 30/300\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 11.4829 - val_loss: 13.3743\n",
      "Epoch 31/300\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 11.7671 - val_loss: 13.2069\n",
      "Epoch 32/300\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 12.6915 - val_loss: 13.0400\n",
      "Epoch 33/300\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 12.1711 - val_loss: 12.9697\n",
      "Epoch 34/300\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 12.2334 - val_loss: 13.0075\n",
      "Epoch 35/300\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 13.3940 - val_loss: 13.7895\n",
      "Epoch 36/300\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 12.6901 - val_loss: 12.8665\n",
      "Epoch 37/300\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 12.9180 - val_loss: 13.0515\n",
      "Epoch 38/300\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 11.6608 - val_loss: 13.2743\n",
      "Epoch 39/300\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 11.0893 - val_loss: 13.3149\n",
      "Epoch 40/300\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 11.9919 - val_loss: 12.9399\n",
      "Epoch 41/300\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 10.7127 - val_loss: 13.1281\n",
      "Epoch 42/300\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 11.9534 - val_loss: 13.4159\n",
      "Epoch 43/300\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 13.4542 - val_loss: 12.7075\n",
      "Epoch 44/300\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 12.1216 - val_loss: 12.8035\n",
      "Epoch 45/300\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 10.6890 - val_loss: 12.8162\n",
      "Epoch 46/300\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 11.5164 - val_loss: 13.2960\n",
      "Epoch 47/300\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 11.7494 - val_loss: 14.0608\n",
      "Epoch 48/300\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 12.0035 - val_loss: 12.8743\n",
      "Epoch 49/300\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 10.3786 - val_loss: 12.7691\n",
      "Epoch 50/300\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 9.7750 - val_loss: 12.9804\n",
      "Epoch 51/300\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 11.1992 - val_loss: 14.5800\n",
      "Epoch 52/300\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 14.3016 - val_loss: 12.4788\n",
      "Epoch 53/300\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 11.7118 - val_loss: 12.5757\n",
      "Epoch 54/300\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 13.0737 - val_loss: 12.5436\n",
      "Epoch 55/300\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 10.7849 - val_loss: 12.2820\n",
      "Epoch 56/300\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 11.1787 - val_loss: 13.2691\n",
      "Epoch 57/300\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 10.8511 - val_loss: 12.5083\n",
      "Epoch 58/300\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 10.3191 - val_loss: 12.8706\n",
      "Epoch 59/300\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 11.1824 - val_loss: 12.7652\n",
      "Epoch 60/300\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 10.5185 - val_loss: 12.6021\n",
      "Epoch 61/300\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 10.8753 - val_loss: 12.4665\n",
      "Epoch 62/300\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 10.1959 - val_loss: 12.1985\n",
      "Epoch 63/300\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 9.7186 - val_loss: 13.0505\n",
      "Epoch 64/300\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 12.0468 - val_loss: 12.0391\n",
      "Epoch 65/300\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 11.3515 - val_loss: 13.0812\n",
      "Epoch 66/300\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 12.6991 - val_loss: 12.1436\n",
      "Epoch 67/300\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 10.1511 - val_loss: 12.5040\n",
      "Epoch 68/300\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 10.2551 - val_loss: 12.6035\n",
      "Epoch 69/300\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 10.4662 - val_loss: 11.9711\n",
      "Epoch 70/300\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 10.6174 - val_loss: 12.4293\n",
      "Epoch 71/300\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 8.9576 - val_loss: 11.9661\n",
      "Epoch 72/300\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 10.3322 - val_loss: 11.9844\n",
      "Epoch 73/300\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 12.2547 - val_loss: 12.1273\n",
      "Epoch 74/300\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 10.9574 - val_loss: 12.4244\n",
      "Epoch 75/300\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 12.7315 - val_loss: 11.8771\n",
      "Epoch 76/300\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 9.5326 - val_loss: 12.1143\n",
      "Epoch 77/300\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 11.0371 - val_loss: 11.8812\n",
      "Epoch 78/300\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 10.4597 - val_loss: 11.8220\n",
      "Epoch 79/300\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 9.0549 - val_loss: 11.4944\n",
      "Epoch 80/300\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 12.5444 - val_loss: 11.6906\n",
      "Epoch 81/300\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 9.0960 - val_loss: 11.4632\n",
      "Epoch 82/300\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 9.2977 - val_loss: 11.3789\n",
      "Epoch 83/300\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 9.8604 - val_loss: 11.8046\n",
      "Epoch 84/300\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 9.0987 - val_loss: 11.4670\n",
      "Epoch 85/300\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 9.8408 - val_loss: 11.1610\n",
      "Epoch 86/300\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 9.7994 - val_loss: 11.2616\n",
      "Epoch 87/300\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 11.1624 - val_loss: 11.6905\n",
      "Epoch 88/300\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 8.6047 - val_loss: 11.7738\n",
      "Epoch 89/300\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 9.8508 - val_loss: 11.4947\n",
      "Epoch 90/300\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 8.7959 - val_loss: 11.1169\n",
      "Epoch 91/300\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 9.2745 - val_loss: 11.5069\n",
      "Epoch 92/300\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 10.6926 - val_loss: 11.2226\n",
      "Epoch 93/300\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 7.9879 - val_loss: 11.3009\n",
      "Epoch 94/300\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 9.2799 - val_loss: 11.5661\n",
      "Epoch 95/300\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 10.3191 - val_loss: 11.2558\n",
      "Epoch 96/300\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 9.0145 - val_loss: 11.1222\n",
      "Epoch 97/300\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 8.3030 - val_loss: 11.1063\n",
      "Epoch 98/300\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 8.6798 - val_loss: 11.9898\n",
      "Epoch 99/300\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 9.2193 - val_loss: 11.1041\n",
      "Epoch 100/300\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 11.1782 - val_loss: 11.4242\n",
      "Epoch 101/300\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 10.4405 - val_loss: 10.9535\n",
      "Epoch 102/300\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 10.1013 - val_loss: 10.7027\n",
      "Epoch 103/300\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 8.5167 - val_loss: 10.9051\n",
      "Epoch 104/300\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 8.8514 - val_loss: 10.9380\n",
      "Epoch 105/300\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 10.1652 - val_loss: 10.7006\n",
      "Epoch 106/300\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 9.0481 - val_loss: 11.1870\n",
      "Epoch 107/300\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 8.3591 - val_loss: 11.4013\n",
      "Epoch 108/300\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 11.1779 - val_loss: 11.6544\n",
      "Epoch 109/300\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 9.7223 - val_loss: 10.7783\n",
      "Epoch 110/300\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 10.1357 - val_loss: 10.8289\n",
      "Epoch 111/300\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 8.7074 - val_loss: 10.6337\n",
      "Epoch 112/300\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 8.6033 - val_loss: 10.7017\n",
      "Epoch 113/300\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 8.7418 - val_loss: 10.6862\n",
      "Epoch 114/300\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 10.7336 - val_loss: 10.4394\n",
      "Epoch 115/300\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 8.1078 - val_loss: 11.3143\n",
      "Epoch 116/300\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 8.9961 - val_loss: 10.5466\n",
      "Epoch 117/300\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 10.4241 - val_loss: 10.4874\n",
      "Epoch 118/300\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 8.1506 - val_loss: 10.3256\n",
      "Epoch 119/300\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 7.9959 - val_loss: 10.8643\n",
      "Epoch 120/300\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 9.3505 - val_loss: 11.2866\n",
      "Epoch 121/300\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 9.8198 - val_loss: 11.1544\n",
      "Epoch 122/300\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 8.6669 - val_loss: 10.8030\n",
      "Epoch 123/300\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 10.1163 - val_loss: 10.3906\n",
      "Epoch 124/300\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 9.4272 - val_loss: 10.6281\n",
      "Epoch 125/300\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 7.9106 - val_loss: 10.1414\n",
      "Epoch 126/300\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 9.5345 - val_loss: 10.9817\n",
      "Epoch 127/300\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 9.2196 - val_loss: 10.0442\n",
      "Epoch 128/300\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 8.9995 - val_loss: 10.5467\n",
      "Epoch 129/300\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 8.0333 - val_loss: 10.6561\n",
      "Epoch 130/300\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 8.0161 - val_loss: 10.4911\n",
      "Epoch 131/300\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 10.4906 - val_loss: 10.2382\n",
      "Epoch 132/300\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 9.1175 - val_loss: 10.2556\n",
      "Epoch 133/300\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 8.8683 - val_loss: 10.1793\n",
      "Epoch 134/300\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 8.2448 - val_loss: 11.2437\n",
      "Epoch 135/300\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 8.2247 - val_loss: 10.3004\n",
      "Epoch 136/300\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 9.8100 - val_loss: 11.1536\n",
      "Epoch 137/300\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 9.8763 - val_loss: 10.6495\n",
      "Epoch 138/300\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 8.6828 - val_loss: 9.9014\n",
      "Epoch 139/300\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 8.7689 - val_loss: 9.9360\n",
      "Epoch 140/300\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 8.7211 - val_loss: 10.4077\n",
      "Epoch 141/300\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 9.7508 - val_loss: 10.4159\n",
      "Epoch 142/300\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 8.6097 - val_loss: 10.3066\n",
      "Epoch 143/300\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 8.2094 - val_loss: 10.0034\n",
      "Epoch 144/300\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 8.5341 - val_loss: 10.8237\n",
      "Epoch 145/300\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 8.2700 - val_loss: 10.7762\n",
      "Epoch 146/300\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 9.2425 - val_loss: 10.1045\n",
      "Epoch 147/300\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 7.8118 - val_loss: 10.1190\n",
      "Epoch 148/300\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 7.7004 - val_loss: 11.9327\n",
      "Epoch 149/300\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 9.2880 - val_loss: 10.7590\n",
      "Epoch 150/300\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 8.2510 - val_loss: 10.9521\n",
      "Epoch 151/300\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 8.3106 - val_loss: 10.4415\n",
      "Epoch 152/300\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 8.1062 - val_loss: 9.9261\n",
      "Epoch 153/300\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 9.4933 - val_loss: 10.5390\n",
      "Epoch 154/300\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 7.3353 - val_loss: 10.1884\n",
      "Epoch 155/300\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 8.7917 - val_loss: 10.4263\n",
      "Epoch 156/300\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 8.6930 - val_loss: 9.9546\n",
      "Epoch 157/300\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 8.8970 - val_loss: 10.2547\n",
      "Epoch 158/300\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 8.6144 - val_loss: 9.9016\n",
      "Epoch 159/300\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 8.7952 - val_loss: 10.9870\n",
      "Epoch 160/300\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 8.3068 - val_loss: 11.3699\n",
      "Epoch 161/300\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 8.3464 - val_loss: 9.7735\n",
      "Epoch 162/300\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 8.3822 - val_loss: 10.2843\n",
      "Epoch 163/300\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 7.8439 - val_loss: 10.9406\n",
      "Epoch 164/300\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 9.1815 - val_loss: 10.3797\n",
      "Epoch 165/300\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 7.4176 - val_loss: 9.9033\n",
      "Epoch 166/300\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 7.6928 - val_loss: 11.6473\n",
      "Epoch 167/300\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 8.0188 - val_loss: 11.4900\n",
      "Epoch 168/300\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 9.1800 - val_loss: 12.3248\n",
      "Epoch 169/300\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 8.4323 - val_loss: 10.3896\n",
      "Epoch 170/300\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 9.3484 - val_loss: 9.9439\n",
      "Epoch 171/300\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 8.1689 - val_loss: 10.0421\n",
      "Epoch 172/300\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 8.7623 - val_loss: 10.5071\n",
      "Epoch 173/300\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 7.2774 - val_loss: 10.1183\n",
      "Epoch 174/300\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 7.8649 - val_loss: 9.7517\n",
      "Epoch 175/300\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 8.0054 - val_loss: 10.4476\n",
      "Epoch 176/300\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 7.6848 - val_loss: 9.6432\n",
      "Epoch 177/300\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 8.3235 - val_loss: 11.1025\n",
      "Epoch 178/300\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 8.3482 - val_loss: 10.3414\n",
      "Epoch 179/300\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 7.6131 - val_loss: 10.5555\n",
      "Epoch 180/300\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 9.2030 - val_loss: 10.5755\n",
      "Epoch 181/300\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 7.7676 - val_loss: 11.0887\n",
      "Epoch 182/300\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 8.2962 - val_loss: 10.7223\n",
      "Epoch 183/300\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 7.9646 - val_loss: 9.9691\n",
      "Epoch 184/300\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 8.6316 - val_loss: 10.2367\n",
      "Epoch 185/300\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 7.2928 - val_loss: 9.8534\n",
      "Epoch 186/300\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 7.0917 - val_loss: 9.9611\n",
      "Epoch 187/300\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 7.7799 - val_loss: 10.3677\n",
      "Epoch 188/300\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 7.9518 - val_loss: 9.8435\n",
      "Epoch 189/300\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 7.5635 - val_loss: 9.8991\n",
      "Epoch 190/300\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 7.0632 - val_loss: 9.9117\n",
      "Epoch 191/300\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 7.9072 - val_loss: 9.8376\n",
      "Epoch 192/300\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 7.1322 - val_loss: 9.7278\n",
      "Epoch 193/300\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 7.8320 - val_loss: 10.5180\n",
      "Epoch 194/300\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 8.2844 - val_loss: 9.6470\n",
      "Epoch 195/300\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 7.1065 - val_loss: 9.7976\n",
      "Epoch 196/300\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 7.8171 - val_loss: 11.2566\n",
      "Epoch 197/300\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 8.1804 - val_loss: 10.9567\n",
      "Epoch 198/300\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 8.2865 - val_loss: 10.2265\n",
      "Epoch 199/300\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 7.3886 - val_loss: 9.8760\n",
      "Epoch 200/300\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 7.6145 - val_loss: 9.9477\n",
      "Epoch 201/300\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 7.4060 - val_loss: 10.0911\n",
      "Epoch 202/300\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 6.9501 - val_loss: 9.6798\n",
      "Epoch 203/300\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 7.0954 - val_loss: 10.4185\n",
      "Epoch 204/300\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 7.2631 - val_loss: 9.6276\n",
      "Epoch 205/300\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 6.8111 - val_loss: 9.9385\n",
      "Epoch 206/300\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 7.8879 - val_loss: 10.2362\n",
      "Epoch 207/300\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 7.0723 - val_loss: 11.2317\n",
      "Epoch 208/300\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 7.8579 - val_loss: 9.7047\n",
      "Epoch 209/300\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 7.4347 - val_loss: 9.7413\n",
      "Epoch 210/300\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 7.9644 - val_loss: 10.2573\n",
      "Epoch 211/300\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 7.7315 - val_loss: 11.1151\n",
      "Epoch 212/300\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 9.5347 - val_loss: 11.1528\n",
      "Epoch 213/300\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 7.7833 - val_loss: 10.4870\n",
      "Epoch 214/300\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 8.2455 - val_loss: 9.7930\n",
      "Epoch 215/300\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 8.1732 - val_loss: 10.4630\n",
      "Epoch 216/300\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 8.0977 - val_loss: 11.1637\n",
      "Epoch 217/300\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 7.7882 - val_loss: 9.6407\n",
      "Epoch 218/300\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 7.1635 - val_loss: 9.5753\n",
      "Epoch 219/300\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 6.7231 - val_loss: 9.8054\n",
      "Epoch 220/300\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 7.3813 - val_loss: 10.4552\n",
      "Epoch 221/300\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 7.1379 - val_loss: 9.4797\n",
      "Epoch 222/300\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 7.7270 - val_loss: 9.6370\n",
      "Epoch 223/300\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 6.3590 - val_loss: 9.8163\n",
      "Epoch 224/300\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 7.1653 - val_loss: 9.7747\n",
      "Epoch 225/300\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 6.6229 - val_loss: 9.5582\n",
      "Epoch 226/300\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 6.2890 - val_loss: 10.3441\n",
      "Epoch 227/300\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 7.9374 - val_loss: 10.0113\n",
      "Epoch 228/300\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 7.4710 - val_loss: 10.3483\n",
      "Epoch 229/300\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 7.0031 - val_loss: 10.0017\n",
      "Epoch 230/300\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 7.2678 - val_loss: 9.7767\n",
      "Epoch 231/300\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 6.8421 - val_loss: 10.3491\n",
      "Epoch 232/300\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 7.7696 - val_loss: 9.9409\n",
      "Epoch 233/300\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 7.4976 - val_loss: 9.6621\n",
      "Epoch 234/300\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 7.6740 - val_loss: 9.6469\n",
      "Epoch 235/300\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 7.2105 - val_loss: 9.9370\n",
      "Epoch 236/300\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 8.2556 - val_loss: 9.7839\n",
      "Epoch 237/300\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 8.0736 - val_loss: 9.8817\n",
      "Epoch 238/300\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 8.0152 - val_loss: 10.3374\n",
      "Epoch 239/300\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 7.6229 - val_loss: 9.5859\n",
      "Epoch 240/300\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 6.7915 - val_loss: 12.2182\n",
      "Epoch 241/300\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 9.2864 - val_loss: 10.7635\n",
      "Epoch 242/300\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 7.1952 - val_loss: 10.6998\n",
      "Epoch 243/300\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 7.3046 - val_loss: 10.0503\n",
      "Epoch 244/300\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 8.1649 - val_loss: 10.0906\n",
      "Epoch 245/300\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 7.8381 - val_loss: 9.9448\n",
      "Epoch 246/300\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 7.3588 - val_loss: 9.8302\n",
      "Epoch 247/300\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 7.8485 - val_loss: 10.7785\n",
      "Epoch 248/300\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 6.9466 - val_loss: 9.6783\n",
      "Epoch 249/300\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 7.2291 - val_loss: 9.7363\n",
      "Epoch 250/300\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 6.3395 - val_loss: 10.1091\n",
      "Epoch 251/300\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 8.2207 - val_loss: 9.8391\n",
      "Epoch 252/300\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 6.9432 - val_loss: 10.0057\n",
      "Epoch 253/300\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 6.9034 - val_loss: 10.2021\n",
      "Epoch 254/300\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 6.3953 - val_loss: 9.9853\n",
      "Epoch 255/300\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 6.5514 - val_loss: 9.9206\n",
      "Epoch 256/300\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 6.6620 - val_loss: 9.3082\n",
      "Epoch 257/300\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 6.3491 - val_loss: 9.9045\n",
      "Epoch 258/300\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 6.8889 - val_loss: 9.7118\n",
      "Epoch 259/300\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 6.3838 - val_loss: 10.1396\n",
      "Epoch 260/300\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 7.5781 - val_loss: 9.4472\n",
      "Epoch 261/300\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 7.2026 - val_loss: 10.0426\n",
      "Epoch 262/300\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 6.4031 - val_loss: 9.8999\n",
      "Epoch 263/300\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 7.2301 - val_loss: 9.5852\n",
      "Epoch 264/300\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 6.3431 - val_loss: 10.3426\n",
      "Epoch 265/300\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 7.5448 - val_loss: 9.4743\n",
      "Epoch 266/300\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 6.4421 - val_loss: 10.3067\n",
      "Epoch 267/300\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 6.9694 - val_loss: 9.9563\n",
      "Epoch 268/300\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 7.5583 - val_loss: 10.0659\n",
      "Epoch 269/300\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 7.2861 - val_loss: 10.1702\n",
      "Epoch 270/300\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 7.1047 - val_loss: 9.5073\n",
      "Epoch 271/300\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 7.2546 - val_loss: 9.8126\n",
      "Epoch 272/300\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 6.4027 - val_loss: 9.5569\n",
      "Epoch 273/300\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 6.1777 - val_loss: 10.3717\n",
      "Epoch 274/300\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 7.0943 - val_loss: 10.1588\n",
      "Epoch 275/300\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 7.2515 - val_loss: 9.8289\n",
      "Epoch 276/300\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 6.7750 - val_loss: 9.9241\n",
      "Epoch 277/300\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 6.8124 - val_loss: 9.9344\n",
      "Epoch 278/300\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 6.3377 - val_loss: 9.8697\n",
      "Epoch 279/300\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 7.1035 - val_loss: 9.6765\n",
      "Epoch 280/300\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 7.5765 - val_loss: 9.3140\n",
      "Epoch 281/300\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 7.1056 - val_loss: 9.4827\n",
      "Epoch 282/300\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 6.5336 - val_loss: 9.7843\n",
      "Epoch 283/300\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 7.9957 - val_loss: 10.0763\n",
      "Epoch 284/300\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 7.8291 - val_loss: 9.9785\n",
      "Epoch 285/300\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 6.0085 - val_loss: 10.0437\n",
      "Epoch 286/300\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 6.3144 - val_loss: 9.7084\n",
      "Epoch 287/300\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 6.5675 - val_loss: 9.6403\n",
      "Epoch 288/300\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 6.7270 - val_loss: 10.1463\n",
      "Epoch 289/300\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 6.2853 - val_loss: 9.9005\n",
      "Epoch 290/300\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 6.5035 - val_loss: 10.3671\n",
      "Epoch 291/300\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 6.7441 - val_loss: 9.1482\n",
      "Epoch 292/300\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 6.2840 - val_loss: 11.8643\n",
      "Epoch 293/300\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 7.6664 - val_loss: 9.7670\n",
      "Epoch 294/300\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 6.1786 - val_loss: 9.3811\n",
      "Epoch 295/300\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 6.3729 - val_loss: 9.9561\n",
      "Epoch 296/300\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 6.3789 - val_loss: 9.9964\n",
      "Epoch 297/300\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 6.3992 - val_loss: 9.5238\n",
      "Epoch 298/300\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 6.9315 - val_loss: 10.3007\n",
      "Epoch 299/300\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 6.7759 - val_loss: 10.1947\n",
      "Epoch 300/300\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 7.1886 - val_loss: 9.7508\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/base.py:1473: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "# gru = create_GRU()\n",
    "# lasso = create_lasso()\n",
    "# lstm = create_LSTM()\n",
    "mlp = create_mlp()\n",
    "rf = create_rf()\n",
    "# ridge = create_ridge()\n",
    "# svm = create_SVM()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import deepforest\n",
    "from deepforest import CascadeForestRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.layers import concatenate\n",
    "\n",
    "\n",
    "# gruPredict = gru.predict(X_train)\n",
    "# lassoPredict = lasso.predict(X_train)\n",
    "# lstmPredict = lstm.predict(X_train)\n",
    "mlpPredict = mlp.predict(X_train)\n",
    "rfPredict = rf.predict(X_train)\n",
    "# ridgePredict = ridge.predict(X_train)\n",
    "# svmPredict = svm.predict(X_train)\n",
    "# lassoPredict.resize(671,1)\n",
    "rfPredict.resize(671,1)\n",
    "# svmPredict.resize(671,1)\n",
    "trainData = concatenate([mlpPredict,rfPredict])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CascadeForestRegressor(n_estimators = 50, criterion='squared_error', backend='sklearn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.23.1'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/utils/validation.py:1310: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-06-27 20:55:52.918] Start to fit the model:\n",
      "[2024-06-27 20:55:52.918] Fitting cascade layer = 0 \n",
      "[2024-06-27 20:56:10.815] layer = 0  | Val MSE = 138.89660 | Elapsed = 17.897 s\n",
      "[2024-06-27 20:56:10.883] Fitting cascade layer = 1 \n",
      "[2024-06-27 20:59:05.750] layer = 1  | Val MSE = 159.88770 | Elapsed = 174.867 s\n",
      "[2024-06-27 20:59:05.750] Early stopping counter: 1 out of 2\n",
      "[2024-06-27 20:59:05.806] Fitting cascade layer = 2 \n",
      "[2024-06-27 21:02:03.061] layer = 2  | Val MSE = 185.95459 | Elapsed = 177.255 s\n",
      "[2024-06-27 21:02:03.061] Early stopping counter: 2 out of 2\n",
      "[2024-06-27 21:02:03.061] Handling early stopping\n",
      "[2024-06-27 21:02:03.108] The optimal number of layers: 1\n"
     ]
    }
   ],
   "source": [
    "model.fit(trainData, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 874us/step\n"
     ]
    }
   ],
   "source": [
    "# gruPredictTest = gru.predict(X_test)\n",
    "# lassoPredictTest = lasso.predict(X_test)\n",
    "# lstmPredictTest = lstm.predict(X_test)\n",
    "mlpPredictTest = mlp.predict(X_test)\n",
    "rfPredictTest = rf.predict(X_test)\n",
    "# ridgePredictTest = ridge.predict(X_test)\n",
    "# svmPredictTest = svm.predict(X_test)\n",
    "# svmPredictTest.resize(168,1)\n",
    "rfPredictTest.resize(168,1)\n",
    "# lassoPredictTest.resize(168,1)\n",
    "testData = concatenate([mlpPredictTest, rfPredictTest])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-06-27 20:47:31.649] Start to evalute the model:\n",
      "[2024-06-27 20:47:31.649] Evaluating cascade layer = 0 \n",
      "[[  4.538048  ]\n",
      " [589.640689  ]\n",
      " [  6.648925  ]\n",
      " [ 53.351529  ]\n",
      " [  7.48610017]\n",
      " [ 87.3450575 ]\n",
      " [ 25.214371  ]\n",
      " [ 59.266652  ]\n",
      " [ 12.669522  ]\n",
      " [  7.3129855 ]\n",
      " [ 44.606091  ]\n",
      " [174.31206   ]\n",
      " [  2.824626  ]\n",
      " [ 15.31132   ]\n",
      " [  5.592785  ]\n",
      " [  9.729625  ]\n",
      " [  3.541738  ]\n",
      " [  3.575182  ]\n",
      " [  8.34600667]\n",
      " [ 80.834858  ]\n",
      " [ 39.34796   ]\n",
      " [  2.015637  ]\n",
      " [  8.38674183]\n",
      " [  3.067232  ]\n",
      " [ 15.4129078 ]\n",
      " [  9.49822817]\n",
      " [ 29.368231  ]\n",
      " [  9.11499067]\n",
      " [104.766778  ]\n",
      " [  7.98100967]\n",
      " [ 14.9861015 ]\n",
      " [  3.894204  ]\n",
      " [ 31.422151  ]\n",
      " [ 45.972941  ]\n",
      " [ 42.198516  ]\n",
      " [ 11.14393   ]\n",
      " [ 17.712453  ]\n",
      " [  8.25065217]\n",
      " [ 12.365369  ]\n",
      " [ 14.9432598 ]\n",
      " [  8.35776567]\n",
      " [ 16.9925275 ]\n",
      " [ 10.4872275 ]\n",
      " [  2.0706    ]\n",
      " [  2.236232  ]\n",
      " [  6.905646  ]\n",
      " [ 21.223399  ]\n",
      " [ 14.5554038 ]\n",
      " [  3.509147  ]\n",
      " [  1.5956    ]\n",
      " [ 24.1190105 ]\n",
      " [ 15.680696  ]\n",
      " [162.622817  ]\n",
      " [ 63.257189  ]\n",
      " [ 34.948722  ]\n",
      " [ 17.153235  ]\n",
      " [ 28.618182  ]\n",
      " [ 12.8275605 ]\n",
      " [ 21.984554  ]\n",
      " [  3.552569  ]\n",
      " [  2.2251    ]\n",
      " [  2.314819  ]\n",
      " [ 39.253482  ]\n",
      " [ 19.2800645 ]\n",
      " [  8.24331167]\n",
      " [ 46.405957  ]\n",
      " [  8.26909567]\n",
      " [  7.113351  ]\n",
      " [ 46.671879  ]\n",
      " [ 89.9956295 ]\n",
      " [  7.86137117]\n",
      " [  2.150743  ]\n",
      " [ 51.709369  ]\n",
      " [ 24.390928  ]\n",
      " [ 50.780253  ]\n",
      " [  2.080322  ]\n",
      " [ 65.405875  ]\n",
      " [  3.687488  ]\n",
      " [ 23.2440325 ]\n",
      " [  1.977834  ]\n",
      " [ 17.856277  ]\n",
      " [  1.986837  ]\n",
      " [  8.604329  ]\n",
      " [ 10.39208017]\n",
      " [ 24.397893  ]\n",
      " [  4.111019  ]\n",
      " [  2.661219  ]\n",
      " [  2.491093  ]\n",
      " [ 55.047724  ]\n",
      " [ 32.967912  ]\n",
      " [ 66.8005425 ]\n",
      " [ 17.103075  ]\n",
      " [  4.866945  ]\n",
      " [  4.501638  ]\n",
      " [ 34.382327  ]\n",
      " [ 98.1728675 ]\n",
      " [ 10.8548675 ]\n",
      " [ 16.624741  ]\n",
      " [ 16.25968   ]\n",
      " [  6.899036  ]\n",
      " [  4.426783  ]\n",
      " [ 37.962483  ]\n",
      " [ 43.298639  ]\n",
      " [  9.58712717]\n",
      " [  3.841539  ]\n",
      " [ 13.180125  ]\n",
      " [ 14.9414118 ]\n",
      " [ 32.519257  ]\n",
      " [101.93426   ]\n",
      " [  4.5986505 ]\n",
      " [  6.099802  ]\n",
      " [ 29.930578  ]\n",
      " [ 15.4267758 ]\n",
      " [ 39.692449  ]\n",
      " [ 26.36275   ]\n",
      " [  9.03851817]\n",
      " [  2.4833    ]\n",
      " [  6.8916955 ]\n",
      " [ 20.5527305 ]\n",
      " [ 15.429726  ]\n",
      " [ 23.697172  ]\n",
      " [  2.766928  ]\n",
      " [ 18.305058  ]\n",
      " [ 14.630192  ]\n",
      " [ 97.7432485 ]\n",
      " [ 11.196055  ]\n",
      " [ 15.7186855 ]\n",
      " [  3.707624  ]\n",
      " [ 12.412164  ]\n",
      " [  4.8462775 ]\n",
      " [  8.604329  ]\n",
      " [ 11.237442  ]\n",
      " [ 20.513176  ]\n",
      " [ 10.929802  ]\n",
      " [ 51.93992   ]\n",
      " [ 16.316589  ]\n",
      " [ 39.995766  ]\n",
      " [ 15.2262088 ]\n",
      " [  5.474246  ]\n",
      " [ 50.511536  ]\n",
      " [  3.405189  ]\n",
      " [  2.992368  ]\n",
      " [  7.56738067]\n",
      " [  6.342544  ]\n",
      " [126.899298  ]\n",
      " [  5.609376  ]\n",
      " [  9.727461  ]\n",
      " [ 94.809177  ]\n",
      " [ 38.937825  ]\n",
      " [589.640689  ]\n",
      " [ 43.10565   ]\n",
      " [ 33.514025  ]\n",
      " [120.213026  ]\n",
      " [ 17.391249  ]\n",
      " [  4.60011   ]\n",
      " [  3.882059  ]\n",
      " [ 17.80682   ]\n",
      " [ 18.444894  ]\n",
      " [ 16.998696  ]\n",
      " [  2.40545   ]\n",
      " [ 28.389286  ]\n",
      " [ 10.20219317]\n",
      " [ 43.474806  ]\n",
      " [ 90.407728  ]\n",
      " [ 30.785541  ]\n",
      " [ 28.287467  ]\n",
      " [  4.299026  ]\n",
      " [ 23.997277  ]]\n"
     ]
    }
   ],
   "source": [
    "res = model.predict(testData)\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9266260875324814\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import r2_score \n",
    "r2 = r2_score(Y_test, res) \n",
    "print(r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.546833614285715\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_absolute_error\n",
    "mae = mean_absolute_error(Y_test,res)\n",
    "print(mae)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5425587782530025\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_absolute_percentage_error\n",
    "mape = mean_absolute_percentage_error(Y_test,res)\n",
    "print(mape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16.566458703067685\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import root_mean_squared_error\n",
    "rmse = root_mean_squared_error(Y_test,res)\n",
    "print(rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[5.91814852e+00 4.68060017e+00]\n",
      " [5.77580811e+02 3.90060699e+02]\n",
      " [7.16818762e+00 6.85659981e+00]\n",
      " [4.41382942e+01 5.15993996e+01]\n",
      " [1.35139828e+01 8.21889973e+00]\n",
      " [8.66902466e+01 8.74632034e+01]\n",
      " [1.49783106e+01 2.96245995e+01]\n",
      " [1.00221451e+02 5.94417992e+01]\n",
      " [1.16826782e+01 1.23114004e+01]\n",
      " [9.42167568e+00 7.58599997e+00]\n",
      " [4.18660774e+01 4.50588989e+01]\n",
      " [1.16405144e+02 1.50219406e+02]\n",
      " [3.98707461e+00 3.30480003e+00]\n",
      " [1.73633232e+01 1.53299999e+01]\n",
      " [5.99115896e+00 5.94189978e+00]\n",
      " [8.68227291e+00 1.04138002e+01]\n",
      " [5.99274778e+00 3.48350000e+00]\n",
      " [4.39484167e+00 3.62089992e+00]\n",
      " [9.15020752e+00 8.11540031e+00]\n",
      " [7.54608078e+01 8.35129013e+01]\n",
      " [3.40919952e+01 3.95985985e+01]\n",
      " [5.37567091e+00 2.74219990e+00]\n",
      " [1.08808985e+01 9.38809967e+00]\n",
      " [5.38325644e+00 3.40709996e+00]\n",
      " [1.63550568e+01 1.52896996e+01]\n",
      " [9.46097088e+00 8.86800003e+00]\n",
      " [1.80455971e+01 3.10678997e+01]\n",
      " [1.19295206e+01 9.86719990e+00]\n",
      " [2.06244385e+02 8.63820038e+01]\n",
      " [7.13566828e+00 8.21770000e+00]\n",
      " [1.12739410e+01 1.38425999e+01]\n",
      " [9.40405178e+00 5.25969982e+00]\n",
      " [2.68376198e+01 3.07712994e+01]\n",
      " [1.91127567e+01 4.90461998e+01]\n",
      " [4.21208115e+01 4.41267014e+01]\n",
      " [1.33813410e+01 1.12746000e+01]\n",
      " [2.75265408e+01 1.85543995e+01]\n",
      " [1.24459658e+01 9.16520023e+00]\n",
      " [1.33319845e+01 1.26920996e+01]\n",
      " [1.61135807e+01 1.51590004e+01]\n",
      " [1.09018059e+01 9.30990028e+00]\n",
      " [2.33527241e+01 1.79906998e+01]\n",
      " [9.01500988e+00 1.01099005e+01]\n",
      " [5.55952311e+00 2.78439999e+00]\n",
      " [4.80843210e+00 2.72090006e+00]\n",
      " [1.40999374e+01 7.10720015e+00]\n",
      " [2.21480007e+01 2.15058002e+01]\n",
      " [1.78252945e+01 1.51281004e+01]\n",
      " [7.17194414e+00 3.63100004e+00]\n",
      " [2.11551070e+00 1.99430001e+00]\n",
      " [2.27083588e+01 2.37182999e+01]\n",
      " [2.17128582e+01 1.71396008e+01]\n",
      " [1.42256638e+02 1.80129395e+02]\n",
      " [8.23505096e+01 6.36095009e+01]\n",
      " [3.38289413e+01 3.35285988e+01]\n",
      " [1.80164375e+01 1.65025005e+01]\n",
      " [2.61426544e+01 2.79172001e+01]\n",
      " [1.05088682e+01 1.21646004e+01]\n",
      " [2.90169296e+01 2.04043007e+01]\n",
      " [5.65629530e+00 4.09079981e+00]\n",
      " [5.09021139e+00 2.72250009e+00]\n",
      " [3.96102786e+00 2.68429995e+00]\n",
      " [3.54784317e+01 4.12210007e+01]\n",
      " [2.48944855e+01 2.10053005e+01]\n",
      " [3.38441772e+01 8.02610016e+00]\n",
      " [6.23194466e+01 4.89131012e+01]\n",
      " [8.98526955e+00 8.26049995e+00]\n",
      " [1.49006557e+01 7.86259985e+00]\n",
      " [5.95140762e+01 4.96363983e+01]\n",
      " [8.96317673e+01 9.10727997e+01]\n",
      " [1.02361021e+01 8.47089958e+00]\n",
      " [5.77836943e+00 2.65159988e+00]\n",
      " [6.42090607e+01 5.24976006e+01]\n",
      " [1.66865463e+01 2.38446999e+01]\n",
      " [5.57578316e+01 5.03907013e+01]\n",
      " [1.87186885e+00 2.36380005e+00]\n",
      " [6.54108810e+01 6.91910019e+01]\n",
      " [5.73671293e+00 4.19799995e+00]\n",
      " [2.33920536e+01 2.28974991e+01]\n",
      " [1.04623580e+00 2.20140004e+00]\n",
      " [1.99149475e+01 1.74309998e+01]\n",
      " [2.79781318e+00 2.21930003e+00]\n",
      " [1.70162621e+01 1.05258999e+01]\n",
      " [1.32951174e+01 1.08018999e+01]\n",
      " [3.36412735e+01 2.48134003e+01]\n",
      " [4.72631788e+00 4.38049984e+00]\n",
      " [5.74460745e+00 3.31550002e+00]\n",
      " [2.51625705e+00 2.64459991e+00]\n",
      " [4.47753906e+01 5.33208008e+01]\n",
      " [4.15698318e+01 3.33306999e+01]\n",
      " [9.24524765e+01 7.70893021e+01]\n",
      " [2.11225891e+01 1.89687004e+01]\n",
      " [6.24214602e+00 5.51280022e+00]\n",
      " [6.28883982e+00 4.91680002e+00]\n",
      " [3.30471954e+01 3.44940987e+01]\n",
      " [8.45250320e+01 9.64728012e+01]\n",
      " [6.31858110e+00 1.28253002e+01]\n",
      " [2.14151287e+01 1.76501007e+01]\n",
      " [1.88667336e+01 1.65990009e+01]\n",
      " [1.10306654e+01 7.61740017e+00]\n",
      " [8.05540371e+00 4.54610014e+00]\n",
      " [4.10350952e+01 4.15774994e+01]\n",
      " [4.43682556e+01 4.37055016e+01]\n",
      " [1.18144064e+01 1.00390997e+01]\n",
      " [5.35840368e+00 4.30100012e+00]\n",
      " [2.31093025e+01 1.51950998e+01]\n",
      " [1.46701469e+01 1.46830997e+01]\n",
      " [3.13855743e+01 3.19754009e+01]\n",
      " [4.93544083e+01 1.07090897e+02]\n",
      " [8.61991787e+00 6.07910013e+00]\n",
      " [1.18022156e+01 6.93680000e+00]\n",
      " [3.45080643e+01 3.02723007e+01]\n",
      " [1.43712816e+01 1.44167995e+01]\n",
      " [7.73586655e+01 4.39756012e+01]\n",
      " [2.76008759e+01 2.61835003e+01]\n",
      " [8.87038517e+00 8.79249954e+00]\n",
      " [4.50723886e+00 2.67810011e+00]\n",
      " [6.50903368e+00 6.75479984e+00]\n",
      " [2.76472340e+01 2.28232002e+01]\n",
      " [2.03559837e+01 1.63764992e+01]\n",
      " [1.69822121e+01 2.19057999e+01]\n",
      " [5.51343870e+00 3.25440001e+00]\n",
      " [2.49291878e+01 2.00268993e+01]\n",
      " [1.17145319e+01 1.35820999e+01]\n",
      " [1.28653870e+02 8.24184036e+01]\n",
      " [2.77027855e+01 1.31113005e+01]\n",
      " [1.24741879e+01 1.43295002e+01]\n",
      " [9.74394131e+00 4.08750010e+00]\n",
      " [2.48202496e+01 1.52648001e+01]\n",
      " [8.74307346e+00 5.39069986e+00]\n",
      " [1.69233971e+01 1.05586004e+01]\n",
      " [9.88383579e+00 1.07923002e+01]\n",
      " [5.35408929e-02 4.22643013e+01]\n",
      " [1.34510746e+01 1.15495005e+01]\n",
      " [6.01281700e+01 5.30601997e+01]\n",
      " [2.62029266e+01 1.84624004e+01]\n",
      " [4.05495949e+01 4.17835999e+01]\n",
      " [1.66593113e+01 1.52684002e+01]\n",
      " [9.19387245e+00 6.15409994e+00]\n",
      " [3.12421150e+01 5.10894012e+01]\n",
      " [5.85519552e+00 3.44180012e+00]\n",
      " [5.94422579e+00 3.02760005e+00]\n",
      " [6.31126547e+00 8.47259998e+00]\n",
      " [1.04323883e+01 7.65619993e+00]\n",
      " [1.44734970e+02 1.43829605e+02]\n",
      " [1.37302713e+01 5.87599993e+00]\n",
      " [2.58074150e+01 1.33941002e+01]\n",
      " [8.09854279e+01 9.11110992e+01]\n",
      " [3.87257156e+01 3.62612000e+01]\n",
      " [3.39144745e+02 4.05702515e+02]\n",
      " [4.36757011e+01 4.34159012e+01]\n",
      " [4.15519562e+01 3.42918015e+01]\n",
      " [8.25748291e+01 1.08824097e+02]\n",
      " [1.72607918e+01 1.59485998e+01]\n",
      " [5.36214495e+00 4.92859983e+00]\n",
      " [5.75725603e+00 4.46350002e+00]\n",
      " [2.57218094e+01 2.06173992e+01]\n",
      " [2.42097244e+01 2.04790993e+01]\n",
      " [3.29290543e+01 1.84409008e+01]\n",
      " [6.07561731e+00 2.64470005e+00]\n",
      " [3.03647232e+01 2.76303005e+01]\n",
      " [1.12126408e+01 1.08858995e+01]\n",
      " [6.89724274e+01 4.46822014e+01]\n",
      " [5.24773903e+01 8.70784988e+01]\n",
      " [3.76108017e+01 3.14004002e+01]\n",
      " [2.97798176e+01 2.71980000e+01]\n",
      " [1.76053715e+01 4.79279995e+00]\n",
      " [1.15286570e+01 2.58096008e+01]]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "The passed model is not callable and cannot be analyzed directly with the given masker! Model: CascadeForestRegressor(n_estimators=50, random_state=1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mshap\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(testData\u001b[38;5;241m.\u001b[39mnumpy())\n\u001b[0;32m----> 3\u001b[0m explainer \u001b[38;5;241m=\u001b[39m \u001b[43mshap\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mExplainer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m shap_values \u001b[38;5;241m=\u001b[39m explainer\u001b[38;5;241m.\u001b[39mshap_values(testData\u001b[38;5;241m.\u001b[39mnumpy())\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/shap/explainers/_explainer.py:174\u001b[0m, in \u001b[0;36mExplainer.__init__\u001b[0;34m(self, model, masker, link, algorithm, output_names, feature_names, linearize_link, seed, **kwargs)\u001b[0m\n\u001b[1;32m    170\u001b[0m             algorithm \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpermutation\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    172\u001b[0m     \u001b[38;5;66;03m# if we get here then we don't know how to handle what was given to us\u001b[39;00m\n\u001b[1;32m    173\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 174\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe passed model is not callable and cannot be analyzed directly with the given masker! Model: \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(model))\n\u001b[1;32m    176\u001b[0m \u001b[38;5;66;03m# build the right subclass\u001b[39;00m\n\u001b[1;32m    177\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m algorithm \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexact\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "\u001b[0;31mTypeError\u001b[0m: The passed model is not callable and cannot be analyzed directly with the given masker! Model: CascadeForestRegressor(n_estimators=50, random_state=1)"
     ]
    }
   ],
   "source": [
    "import shap\n",
    "print(testData.numpy())\n",
    "explainer = shap.Explainer(model)\n",
    "shap_values = explainer.shap_values(testData.numpy())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.40095557 0.59904443]\n"
     ]
    }
   ],
   "source": [
    "print(model.get_layer_feature_importances(0))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
